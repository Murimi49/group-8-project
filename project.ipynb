{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X entiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Sentiment analysis is a technique used to determine the emotional tone from remarks or comments that users of certain application either X, facebook, instagram or tiktok about a certain topic.\n",
    "\n",
    "This remarks can have a positive tone, neutral tone and negative tone towards a certain audience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "### Background\n",
    "X is a popular social media platform that most people use to share their opinions about certain topics, this can either be political, religious or social trends.\n",
    "\n",
    "These opinions can be positive, negative or neutral towards the topic been addressed. These opinions are mainly in text format.\n",
    "\n",
    "\n",
    "### Problem statement\n",
    "Due to the growing amount of user-generated content on X, it is becoming more difficult for researchers, corporation to accurately and efficiently gauge public opinion on particular subjects, goods, or occasions. It's challenging to glean useful data from tweets because of their sheer number and informal style, which frequently includes slang, acronyms, and emoticons. This leads to inadequate insights generated by corporations from public opinion to improve certain products or when advertising certain products.\n",
    "\n",
    "### Objective\n",
    "1. Develop a sentiment analysis model that uses natural preprocessing language(nlp) to preprocess and clean the tweets, and make it in a more structured format for sentiment analysis.\n",
    "\n",
    "2. Use the sentiment analysis model that can accurately classify tweets into positive, negative and neutral sentiment categories.\n",
    "\n",
    "3. Evaluate Performance: Measure the model's accuracy, precision, recall, and F1-score on a labeled dataset, and iteratively improve based on evaluation results.\n",
    "\n",
    "### Conclusion\n",
    "Although emojis, acronyms, and slang are common on Twitter, the informal tone of the network makes it difficult to draw meaningful conclusions from user-generated content. While sophisticated NLP models and neural networkÂ approaches, handle the complexity of sentiment grouping, preprocessing is essential to cleanse this noisy data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (3.6.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (5.3.1)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (1.1.3)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Collecting huggingface-hub>=0.22.0\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (20.4)\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from datasets) (1.18.5)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.17-py38-none-any.whl (132 kB)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-17.0.0-cp38-cp38-win_amd64.whl (25.2 MB)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (3.0.4)\n",
      "Requirement already satisfied: yarl<1.6.0,>=1.0 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (1.5.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (20.2.0)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from aiohttp->datasets) (4.7.5)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging->datasets) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests>=2.32.2->datasets) (2.10)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\natali\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests>=2.32.2->datasets) (1.25.10)\n",
      "Installing collected packages: tqdm, dill, xxhash, charset-normalizer, requests, typing-extensions, fsspec, filelock, huggingface-hub, multiprocess, pyarrow, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.50.2\n",
      "    Uninstalling tqdm-4.50.2:\n",
      "      Successfully uninstalled tqdm-4.50.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 0.15.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "fbprophet 0.7.1 requires cmdstanpy==0.9.5, which is not installed.\n",
      "google-api-core 1.22.2 requires protobuf>=3.12.0, but you'll have protobuf 3.11.2 which is incompatible.\n",
      "huggingface-hub 0.25.1 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n",
      "multiprocess 0.70.17 requires dill>=0.3.9, but you'll have dill 0.3.8 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Uninstalling pyarrow-0.15.1:\n",
      "      Successfully uninstalled pyarrow-0.15.1\n",
      "Successfully installed charset-normalizer-3.3.2 datasets-3.0.1 dill-0.3.8 filelock-3.16.1 fsspec-2024.6.1 huggingface-hub-0.25.1 multiprocess-0.70.17 pyarrow-17.0.0 requests-2.32.3 tqdm-4.66.5 typing-extensions-4.12.2 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1374ac2ad0848658b43bae159910427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NATALI\\anaconda3\\envs\\learn-env\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NATALI\\.cache\\huggingface\\hub\\datasets--mteb--tweet_sentiment_extraction. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3103e71600494fb5baa180221d0cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/3.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe5b4e080ea408385012d96bf8fec5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/465k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b35b0da29ea4f81b4757043d645cc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/27481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea46b20a6a44edaa1bdcf283923d466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3534 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"mteb/tweet_sentiment_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16fab9f95b</td>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74a76f6e0a</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04dd1d2e34</td>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bbe3cbf620</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8a939bfb59</td>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3440297f8b</td>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>919fa93391</td>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>af3fed7fc3</td>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40e7becabf</td>\n",
       "      <td>Hes just not that into you</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  label  \\\n",
       "0   cb774db0d1                I`d have responded, if I were going      1   \n",
       "1   549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2   088c60f138                          my boss is bullying me...      0   \n",
       "3   9642c003ef                     what interview! leave me alone      0   \n",
       "4   358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...      1   \n",
       "6   6e0c6d75b1  2am feedings for the baby are fun when he is a...      2   \n",
       "7   50e14c0bb8                                         Soooo high      1   \n",
       "8   e050245fbd                                        Both of you      1   \n",
       "9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....      2   \n",
       "10  2339a9b08b   as much as i love to be hopeful, i reckon the...      1   \n",
       "11  16fab9f95b  I really really like the song Love Story by Ta...      2   \n",
       "12  74a76f6e0a       My Sharpie is running DANGERously low on ink      0   \n",
       "13  04dd1d2e34  i want to go to music tonight but i lost my vo...      0   \n",
       "14  bbe3cbf620                         test test from the LG enV2      1   \n",
       "15  8a939bfb59                              Uh oh, I am sunburned      0   \n",
       "16  3440297f8b   S`ok, trying to plot alternatives as we speak...      0   \n",
       "17  919fa93391  i`ve been sick for the past few days  and thus...      0   \n",
       "18  af3fed7fc3         is back home now      gonna miss every one      0   \n",
       "19  40e7becabf                         Hes just not that into you      1   \n",
       "\n",
       "   label_text  \n",
       "0     neutral  \n",
       "1    negative  \n",
       "2    negative  \n",
       "3    negative  \n",
       "4    negative  \n",
       "5     neutral  \n",
       "6    positive  \n",
       "7     neutral  \n",
       "8     neutral  \n",
       "9    positive  \n",
       "10    neutral  \n",
       "11   positive  \n",
       "12   negative  \n",
       "13   negative  \n",
       "14    neutral  \n",
       "15   negative  \n",
       "16   negative  \n",
       "17   negative  \n",
       "18   negative  \n",
       "19    neutral  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data = dataset['train']\n",
    "# Convert the dataset to a DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          27481 non-null  object\n",
      " 1   text        27481 non-null  object\n",
      " 2   label       27481 non-null  int64 \n",
      " 3   label_text  27481 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 858.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
