# X Sentiment Analysis Using CNN

## Introduction

Sentiment analysis is a technique used to determine the emotional tone from remarks or comments that users of certain application either X, facebook, instagram or tiktok about a certain topic.

This remarks can have a positive tone, neutral tone and negative tone towards a certain audience.

## Business Understanding

### Background

The way individuals engage and communicate has changed dramatically as a result of the social media sites like X (previously Twitter) growing so quickly. These platforms are being used more and more as a gauge of public sentiment on a range of issues, including goods and services. Businesses frequently use social media to assess customer opinion and guide their marketing initiatives.

However, there are many obstacles in the way of gleaning valuable insights from social media data. The amount of content combined with its informal and frequently colloquial style makes it challenging for computers to comprehend and process. Slang, acronyms, and emoticons can all be used to make sentiment analysis more difficult.

Principal Difficulties in Sentiment Analysis of Social Media:
1. Informal Language: Slang, acronyms, and emoticons are examples of the informal language used in social media posts, which can make it challenging for computers to understand what is being said.

2. Ambiguity: Posts on social media are not exempt from the inherent ambiguity of human language. Depending on the context, one word or phrase can mean different things to different people.

3. Subjectivity: Personal prejudices, cultural disparities, and other elements can all have an impact on sentiment. It might be difficult for computers to fully represent the subtleties of human emotions.


### Problem statement
X or formerly known as twitter is a growing social media platform, that many of its users use to make their opinions based on the topic of discussion. These opinions might be negative, positive and neutral depending on how people perceive it. As a business, sometimes corporations tend to advertise their products on this social media platform in order to get public opinion regarding on the product been advertised.

Users of this platform write their opinions the way they want to and sometimes it is challenging to get useful data from the tweets because of the sheer number and informal style, which frequently includes slang, acronyms, and emoticons. Many corporations rely on using computers as the easiest and fastest way of retrieving useful data instead of human labor.

Sometimes the computer cannot comprehend the tweets because of their informal style and make it difficult to fetch useful data needed by the corporations. Another issue is that computers cannot differentiate between positive, negative or neutral tweets just based on input text alone as that is only perceived by humans alone. This leads to inadequate insights generated by corporations from public opinion to improve certain products or when advertising certain products.

### Objective

1. Develop a sentiment analysis model that uses natural preprocessing language(nlp) to preprocess and clean the tweets, and make it in a more structured format for sentiment analysis.

2. Use the sentiment analysis model that can accurately classify tweets into positive, negative and neutral sentiment categories.

3. Evaluate Performance: Measure the model's accuracy, precision, recall, and F1-score on a labeled dataset, and iteratively improve based on evaluation results.

### Conclusion
More advanced sentiment analysis methods must be developed due to the increasing complexity of social media data. Computers can be efficient and scalable, but they frequently have trouble capturing the subtleties of human emotion and language. Researchers and companies need to invest in cutting-edge algorithms and models that can comprehend the subjectivity, tone, and context of social media posts in order to get beyond these restrictions. Organizations may improve their marketing tactics, make better judgements, and obtain deeper insights into public opinion by developing their sentiment analysis capabilities.

## Data Undarsatnding

In this section, we delve into the dataset used for sentiment analysis, examining its structure, content, and relevant statistics to better understand the information it contains and how it can be leveraged for model training.

### Dataset Overview

The dataset comprises tweets extracted from the social media platform X (formerly Twitter), labeled with three sentiment categories: positive, negative, and neutral. The objective is to classify these tweets into their respective categories based on the sentiment expressed.

### Data Structure
The dataset consists of the following columns:

- id: A unique identifier for each tweet.

- text: The content of the tweet, which may contain slang, emojis, and informal language typical of social media interactions.

- label: A numerical representation of the sentiment, where:

    0- represents negative sentiment

    1- represents neutral sentiment

    2- represents positive sentiment

- label_text: A textual representation of the sentiment label (i.e., "positive", "negative", "neutral").

### Summary Statistics

An overview of the dataset reveals key statistics regarding the number of entries and class distribution:

Total number of tweets: 27481

Class Distribution:

    Neutral: 11118

    Negative: 7781

    Positive: 8582

**Class Distribution**

![alt text](/images/image.png)


This bar chart illustrates the distribution of sentiment classes in the dataset. We observe that the dataset is slightly imbalanced, with more neutral tweets compared to positive and negative tweets.

Test data

![alt text](images/image5.png)


This bar chart illustrates the distribution of sentiment classes in the dataset. We observe that the dataset is slightly imbalanced, with more neutral tweets compared to positive and negative tweets.


## Data Preparation

### Text Preprocessing

The text preprocessing step aims to clean the raw text data to ensure it is in a suitable format for modeling. This involves:

- Lowercasing all the text to ensure uniformity.

- Removing special characters, punctuation, and URLs that are not useful for sentiment classification.

- Tokenization, where each tweet is broken down into individual words or tokens.

- Stopwords removal, eliminating common words (like "the", "is", etc.) that don't carry much semantic value.

- Regular expressions, to remove html tags, elongated words, possessives, mentions, html links and contractions.

- Padding, where we ensure that each sequence (tweet) has the same length by adding zeros to shorter tweets or truncating longer ones. This helps standardize the input for machine learning models.

### Tokenization and Padding

After preprocessing, we convert the cleaned text data into sequences of numerical tokens using a tokenizer. Each word in the dataset is assigned a unique integer value. This transformation allows us to feed the data into machine learning models.

Given that tweet lengths can vary significantly, we apply padding to make all sequences uniform.

**Distribution of Sequence Lengths**.

For the train data

![alt text](images/image3.png)

The histogram shows the distribution of tweet lengths in terms of the number of tokens. Most tweets in the dataset contain fewer than 10 tokens, with the majority being between 5 and 10 tokens long.

For the test data

![alt text](images/image4.png)


### CNN Modeling
#### Model 1

![alt text](images/image-6.png)

![alt text](images/image-7.png)

From the visualizations, it shows that the model is overfitting as the accuracy keeps on increasing as the validation accuracy remains static.

Test Loss: 3.0579
Test Accuracy: 36.98%

To conclude, we can say that model 1 is not fit to be used to make predictions on the test data. This is because model 1 is overfitting based on the high accuracy on the training data and the low accuracy on the test data. 

#### Model 2


![alt text](images/image-8.png)

![alt text](images/image-9.png)

Decreasing Validation Loss: During the training process, there is a constant decrease in the validation loss. This is encouraging since it shows that the model is becoming more effective with data that hasn't been seen yet.

Reducing Training Loss: As would be predicted throughout training, there is a reduction in training loss as well. The model appears to be effectively learning the fundamental patterns in the data and generalizing, based on the trend of both training and validation loss declining.

Growing Validation Accuracy: Throughout the training phase, the validation accuracy keeps rising. This is encouraging since it shows that the model is becoming more effective with data that hasn't been seen yet.

Decreasing Training Accuracy: Following a high, the training accuracy is now declining. This could indicate that the model is beginning to overfit the training set, picking up on its quirks rather than broad patterns.


Test Loss: 1.0870
Test Accuracy: 40.46%

Model 2 is performing better than model 1 in terms of test accuracy and test loss on the test data 


#### Model 3

![alt text](images/image-10.png)

![alt text](images/image-11.png)

Growing Validation Loss: Throughout the training process, the validation loss keeps rising. This is a clear sign that overfitting is occurring. The model is not able to generalise to previously unknown data since it is learning the training set too well.

Reducing Training Loss: During training, it is to be expected that the training loss will decrease. The model may not be becoming better at handling fresh data, though, if the declining trend of the training loss is not matched by a declining validation loss.

Growing Validation Accuracy: Throughout the training phase, the validation accuracy keeps rising. This is encouraging since it shows that the model is becoming more effective with data that hasn't been seen yet.

Decreasing Training Accuracy: Following a high, the training accuracy is now declining. This could indicate that the model is beginning to overfit the training set, picking up on its quirks rather than broad trends.

Test Loss: 1.0977
Test Accuracy: 40.44%

With comparison with the other 2 models, model 3 is performing better than model in terms of evaluating the test data but slightly poorly than model 2. It although has a higher training accuracy compared to model 2.

#### Conclusion

In conclusion, our model was able to classify positive tweets from negatives and vice versa. This shows that our model given an input text can be able to classify positive and negative tweets.









